The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100
Value Iteration,33,16,35,43,28,29,14,30,33,39
Policy Iteration,31,22,37,33,39,15,41,39,38,33
Q Learning,68,40,62,39,46,58,42,41,50,57

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100
Value Iteration,106,193,252,281,304,353,409,507,530,569
Policy Iteration,143,231,308,415,512,592,772,792,889,1009
Q Learning,2354,3827,5165,6430,7638,8858,10082,11285,12499,13760

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,10,20,30,40,50,60,70,80,90,100
Value Iteration Rewards,88.65,-54.75,85.7,83.15,91.15,93.55000000000001,-56.8,92.2,92.65,86.30000000000001
Policy Iteration Rewards,93.85000000000001,-58.6,92.25,91.9,86.55000000000001,-57.9,91.10000000000001,90.55000000000001,83.15,91.9
Q Learning Rewards,41.64999999999994,71.19999999999997,62.74999999999994,76.54999999999997,65.34999999999997,60.899999999999956,84.50000000000001,71.84999999999997,71.44999999999996,62.24999999999996
